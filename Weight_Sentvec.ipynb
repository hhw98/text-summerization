{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import jieba\n",
    "from gensim.models import FastText\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(text):\n",
    "    #remove strange character\n",
    "    text = text.replace(\"\\r\\n\",\"\")\n",
    "    #split sentences\n",
    "    sentences = re.split('([。?!！？.])', text)\n",
    "    new_sentences = []\n",
    "    for i in range(len(sentences)//2):\n",
    "        sent = sentences[2*i] + sentences[2*i+1]\n",
    "        new_sentences.append(sent)\n",
    "    return new_sentences\n",
    "\n",
    "def get_stop_words(stopwords_file):\n",
    "    stop_words = []\n",
    "    for line in open(stopwords_file, 'r', encoding='utf-8'):\n",
    "        stop_words.append(line.replace('\\n', ''))\n",
    "    return stop_words\n",
    "\n",
    "def get_tokens(sent):\n",
    "    #remove punctuation\n",
    "    sent = re.findall(r'[\\d|\\w]+', sent)\n",
    "    stopwords = get_stop_words(\"stopwords.txt\")\n",
    "    words = list(jieba.cut(\"\".join(sent)))\n",
    "    #remove stopwords\n",
    "    words = list(set(words) - set(stopwords))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_freq(text):\n",
    "    sentences = split_sentence(text) \n",
    "    \n",
    "    word_lists = [list(jieba.cut(\"\".join(re.findall(r'[\\d|\\w]+', sentence)))) for sentence in sentences]\n",
    "    \n",
    "    words = [word for word_list in word_lists for word in word_list]\n",
    "    \n",
    "    word_freq = {w: c for w, c in Counter(words).items()}\n",
    "    \n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD_modify(sentences_vec, npc=1):\n",
    "    \n",
    "    sentences_vec = np.array(sentences_vec)\n",
    "    #print(sentences_vec)\n",
    "    #print(sentences_vec.shape)\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(sentences_vec)\n",
    "    u = svd.components_\n",
    "    sentences_vec -= u.dot(u.transpose()) * sentences_vec\n",
    "    return sentences_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, frequency, word2vec):\n",
    "    \n",
    "    alpha = 1e-4\n",
    "    \n",
    "    sent = re.findall(r'[\\d|\\w]+', sentence)\n",
    "    \n",
    "    words =jieba.cut(\"\".join(sent)) \n",
    "    \n",
    "    token_freq = {w: c for w, c in Counter(words).items()}\n",
    "    \n",
    "    high_token_freq = max(token_freq.values())\n",
    "    \n",
    "    high_freq = max(frequency.values())\n",
    "    \n",
    "    tokens = get_tokens(sentence)\n",
    "    \n",
    "    sentence_vec = np.zeros_like(word2vec.wv[\"跳舞\"])\n",
    "    \n",
    "    for word in tokens:\n",
    "        \n",
    "        weight = alpha*token_freq.get(word, high_token_freq) /(alpha+frequency.get(word, high_freq))\n",
    "        \n",
    "        word_vec = weight * word2vec.wv[word]\n",
    "        \n",
    "        sentence_vec += word_vec\n",
    "        \n",
    "    sentence_vec /= len(tokens)\n",
    "    \n",
    "\n",
    "    return sentence_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_distance(text, title):\n",
    "    \n",
    "    #get sentence vector list\n",
    "    sentences = split_sentence(text)\n",
    "    \n",
    "    word_freq = document_freq(text)\n",
    "    \n",
    "    print(\"Get sentence embedding...\")\n",
    "    \n",
    "    print(\"Load model...\")\n",
    "    \n",
    "    word2vec = FastText.load(\"../fasttext.model\")\n",
    "    \n",
    "    print(\"Model is Done!\")\n",
    "    \n",
    "    sentences_vec = [sentence_embedding(sentence, word_freq, word2vec) for sentence in sentences]\n",
    "    \n",
    "    print(\"sentence embedding is Done!\")\n",
    "    \n",
    "    if title:\n",
    "        sentences_vec += sentence_embedding(title, word_freq, word2vec)\n",
    "    \n",
    "    sentences_vec = SVD_modify(sentences_vec)\n",
    "    \n",
    "    #the mean of sentences vector is text vector\n",
    "    text_vec = np.mean(sentences_vec)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if title:\n",
    "        sentences_vec = sentences_vec[1:]\n",
    "        \n",
    "    #get cosine\n",
    "    sentences_cos = {}\n",
    "    for iid, sentence_vec in enumerate(sentences_vec):\n",
    "        sentences_cos[iid] = cosine(sentence_vec, text_vec)\n",
    "    return sentences_cos, sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_ranking(text, title):\n",
    "    \n",
    "    print(\"Get sentence cos distance...\")\n",
    "    \n",
    "    sentences_cos, sentences = sentences_distance(text, title)\n",
    "\n",
    "    print(\"sentence cos distance is done!\")\n",
    "    \n",
    "    #first sentence with reduce distance to half\n",
    "    sentences_cos[0] /= 2\n",
    "    \n",
    "    sentences_cos = sorted(sentences_cos.items(), key = lambda x:x[1])\n",
    "    \n",
    "    return sentences_cos, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization_by_w2v_weight(text, title, summary_ratio):\n",
    "    \n",
    "    print(\"Get ranking of sentences...\")\n",
    "    \n",
    "    sentences_ranking_id, sentences = sentences_ranking(text, title)\n",
    "    \n",
    "    print(\"ranking of sentences is done!\")\n",
    "    \n",
    "    sentences_candidate = [sentence_id[0] for sentence_id in sentences_ranking_id[:len(sentences) // summary_ratio]]\n",
    "    \n",
    "    return \" \".join([sentences[iid] for iid in sorted(sentences_candidate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get ranking of sentences...\n",
      "Get sentence cos distance...\n",
      "Get sentence embedding...\n",
      "Load model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyh6hhy\\Miniconda3\\envs\\stanfordnlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is Done!\n",
      "sentence embedding is Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyh6hhy\\Miniconda3\\envs\\stanfordnlp\\lib\\site-packages\\scipy\\spatial\\distance.py:720: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence cos distance is done!\n",
      "ranking of sentences is done!\n",
      "网易娱乐7月21日报道林肯公园主唱查斯特·贝宁顿 Chester Bennington于今天早上,在洛杉矶帕洛斯弗迪斯的一个私人庄园自缢身亡,年仅41岁。 此消息已得到洛杉矶警方证实。 林肯公园另一位主唱麦克信田确认了 Chester Bennington自杀属实,并对此感到震惊和心痛,称稍后官方会发布声明。\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    text = '网易娱乐7月21日报道林肯公园主唱查斯特·贝宁顿 Chester Bennington于今天早上,在洛杉矶帕洛斯弗迪斯的一个私人庄园自缢身亡,年仅41岁。此消息已得到洛杉矶警方证实。洛杉矶警方透露, Chester的家人正在外地度假, Chester独自在家,上吊地点是家里的二楼。一说是一名音乐公司工作人员来家里找他时发现了尸体,也有人称是佣人最早发现其死亡。林肯公园另一位主唱麦克信田确认了 Chester Bennington自杀属实,并对此感到震惊和心痛,称稍后官方会发布声明。Chester昨天还在推特上转发了一条关于曼哈顿垃圾山的新闻。粉丝们纷纷在该推文下留言,不相信 Chester已经走了。外媒猜测,Chester选择在7月20日自杀的原因跟他极其要好的朋友Soundgarden(声音花园)乐队以及AudioslaveChris乐队主唱 Cornell有关,因为7月20日是 Chris CornellChris的诞辰。而 Cornell于今年5月17日上吊自杀,享年52岁。 Chris去世后, Chester还为他写下悼文。对于 Chester的自杀,亲友表示震惊但不意外,因为 Chester曾经透露过想自杀的念头,他曾表示自己童年时被虐待,导致他医生无法走出阴影,也导致他长期酗酒和嗑药来疗伤。目前,洛杉矶警方仍在调查Chester的死因。据悉, Chester与毒品和酒精斗争多年,年幼时期曾被成年男子性侵,导致常有轻生念头。 Chester生前有过2段婚姻,育有6个孩子。林肯公园在今年五月发行了新专辑《多一丝曙光OneMoreLight》,成为他们第五张登顶ilboard排行榜的专辑。而昨晚刚刚发布新单《 Talking To Myself》MV'\n",
    "    #text = '123'\n",
    "    title = None\n",
    "    summary_ratio = 4\n",
    "    summarization = get_summarization_by_w2v_weight(text, title, summary_ratio)\n",
    "    print(summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
