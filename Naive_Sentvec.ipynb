{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyh6hhy\\Miniconda3\\envs\\stanfordnlp\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_sentence(text):\n",
    "    #remove strange character\n",
    "    text = text.replace(\"\\r\\n\",\"\")\n",
    "    #split sentences\n",
    "    sentences = re.split('([。?!！？.])', text)\n",
    "    new_sentences = []\n",
    "    for i in range(len(sentences)//2):\n",
    "        sent = sentences[2*i] + sentences[2*i+1]\n",
    "        new_sentences.append(sent)\n",
    "    return new_sentences\n",
    "def get_stop_words(stopwords_file):\n",
    "    stop_words = []\n",
    "    for line in open(stopwords_file, 'r', encoding='utf-8'):\n",
    "        stop_words.append(line.replace('\\n', ''))\n",
    "    return stop_words\n",
    "\n",
    "#Chinese\n",
    "def get_tokens(sent):\n",
    "    #remove punctuation\n",
    "    sent = re.findall(r'[\\d|\\w]+', sent)\n",
    "    stopwords = get_stop_words(\"stopwords.txt\")\n",
    "    words = list(jieba.cut(\"\".join(sent)))\n",
    "    #remove stopwords\n",
    "    words = list(set(words) - set(stopwords))\n",
    "    return words\n",
    "\n",
    "#English\n",
    "def get_token(sent):\n",
    "\n",
    "    stopwords = get_stop_words(\"stopwords.txt\")\n",
    "\n",
    "    words = list(sent.split())\n",
    "    #remove stopwords\n",
    "    words = list(set(words) - set(stopwords))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_cos(text, title):\n",
    "    word2vec = FastText.load(\"../fasttext.model\")\n",
    "    #preprocess text\n",
    "    sentences = split_sentence(text)\n",
    "    tokens = [get_tokens(sentence) for sentence in sentences]\n",
    "            \n",
    "    #text vector and title vector\n",
    "    text_vec = np.zeros_like(word2vec.wv[\"测试\"])\n",
    "    sents_vec = []\n",
    "    \n",
    "    for i, words in enumerate(tokens):\n",
    "        sent_vec = np.zeros_like(word2vec.wv[\"测试\"])\n",
    "        for word in words:\n",
    "            sent_vec += word2vec.wv[word]\n",
    "            sents_vec.append(sent_vec)\n",
    "        #first sentence will add two more times\n",
    "        if i == 0:\n",
    "            text_vec += sent_vec*3\n",
    "        else:\n",
    "            text_vec += sent_vec\n",
    "            \n",
    "    #preprocess title\n",
    "    title_flag = False\n",
    "    title_vec = np.zeros_like(word2vec.wv[\"测试\"])\n",
    "    if title:\n",
    "        title_flag = True\n",
    "        title_tokens = get_tokens(title)\n",
    "        title_vec = np.sum([word2vec.wv(word) for word in title_tokens])/len(title_tokens)\n",
    "    #add 5 more times title vector\n",
    "    text_vec += title_vec*3\n",
    "    #text vector\n",
    "    text_vec = text_vec/(len(sentences)+5)\n",
    "    \n",
    "    #get cos between text and each sentence\n",
    "    sentences_cos_id = {}\n",
    "    for i, sent in enumerate(sents_vec):\n",
    "        sentences_cos_id[i] = cosine(sent, text_vec)\n",
    "    return sentences_cos_id, sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_ranking(text, title):\n",
    "    \n",
    "    sentences_cos_id, sentences = get_sentence_cos(text, title)\n",
    "    #first sentence with reduce distance to half\n",
    "    sentences_cos_id[0] /= 2\n",
    "    sorted_sentences_cos_id = sorted(sentences_cos_id.items(), key = lambda x: x[1], reverse = False)\n",
    "    \n",
    "    return sorted_sentences_cos_id, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization_by_naive_sentvec(text, title, summary_ratio):\n",
    "    if text == None:\n",
    "        print(\"please input text\")\n",
    "        return None\n",
    "    \n",
    "    sentences_cos_id, sentences = sentences_ranking(text, title)\n",
    "\n",
    "    sentences_candidate = sentences_cos_id[:(len(sentences)//summary_ratio)]\n",
    "    \n",
    "    return \" \".join([sentences[sents_id[0]] for sents_id in sorted(sentences_candidate)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I took EECS 545 machine learning class last semester.  I implement different kinds of supervised machine learning algorithm step by step in python.  In addition, I have some deep learning experiences with machine learning framework, like PyTorch and Tensorflow.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = \"网易娱乐7月21日报道林肯公园主唱查斯特·贝宁顿 Chester Bennington于今天早上,在洛杉矶帕洛斯弗迪斯的一个私人庄园自缢身亡,年仅41岁。此消息已得到洛杉矶警方证实。洛杉矶警方透露, Chester的家人正在外地度假, Chester独自在家,上吊地点是家里的二楼。一说是一名音乐公司工作人员来家里找他时发现了尸体,也有人称是佣人最早发现其死亡。林肯公园另一位主唱麦克信田确认了 Chester Bennington自杀属实,并对此感到震惊和心痛,称稍后官方会发布声明。Chester昨天还在推特上转发了一条关于曼哈顿垃圾山的新闻。粉丝们纷纷在该推文下留言,不相信 Chester已经走了。外媒猜测,Chester选择在7月20日自杀的原因跟他极其要好的朋友Soundgarden(声音花园)乐队以及AudioslaveChris乐队主唱 Cornell有关,因为7月20日是 Chris CornellChris的诞辰。而 Cornell于今年5月17日上吊自杀,享年52岁。 Chris去世后, Chester还为他写下悼文。对于 Chester的自杀,亲友表示震惊但不意外,因为 Chester曾经透露过想自杀的念头,他曾表示自己童年时被虐待,导致他医生无法走出阴影,也导致他长期酗酒和嗑药来疗伤。目前,洛杉矶警方仍在调查Chester的死因。据悉, Chester与毒品和酒精斗争多年,年幼时期曾被成年男子性侵,导致常有轻生念头。 Chester生前有过2段婚姻,育有6个孩子。林肯公园在今年五月发行了新专辑《多一丝曙光OneMoreLight》,成为他们第五张登顶ilboard排行榜的专辑。而昨晚刚刚发布新单《 Talking To Myself》MV\"\n",
    "    title = None\n",
    "    summary_ratio = 2\n",
    "    text = \"I took EECS 545 machine learning class last semester. I implement different kinds of supervised machine learning algorithm step by step in python. For example, kNN classifier, Regularized  Linear  Regression, Weighted Linear Regression, logistic regression, ridge regression, Support Vector Regression, Bayesian, Gradient and Coordinate Decent and Neural Nets. And some supervised machine learning algorithm, like LDA, QDA, K-means, pca. Thus, I am not only familiar how to use machine learning model from some packages, like sklearn, but also know about how to implement in python and mathematic  deduction behind different model. In addition, I have some deep learning experiences with machine learning framework, like PyTorch and Tensorflow. \"\n",
    "    summarization = get_summarization_by_naive_sentvec(text, title, summary_ratio)\n",
    "    print(summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
